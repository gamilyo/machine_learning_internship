{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import librosa\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1 = 'spk1'\n",
    "data_path_2 = 'spk2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speeches(path):\n",
    "    waves = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
    "    labels = []\n",
    "    samples_rate = []\n",
    "    all_waves = []\n",
    "    for wav in waves:\n",
    "        sample_rate, samples = wavfile.read(join(path,wav))\n",
    "        samples_rate.append(sample_rate)\n",
    "        labels.append(wav[len(wav)-5])\n",
    "        all_waves.append(samples)\n",
    "    return all_waves ,samples_rate,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrograms(waves):\n",
    "    sample_rate = 8000\n",
    "    spectros = []\n",
    "    freqs = []\n",
    "    tims = []\n",
    "    for wav in waves:\n",
    "        frequencies, times, spectrogram = signal.spectrogram(wav, sample_rate)\n",
    "        freqs.append(frequencies)\n",
    "        tims.append(times)\n",
    "        spectros.append(spectrogram)\n",
    "    return freqs,tims,spectros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_waves_1,samples_rate_1,labels_1 = load_speeches(data_path_1)\n",
    "\n",
    "all_waves_2,samples_rate_2,labels_2 = load_speeches(data_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91601, 107760)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len_1 = max([len(x) for x in all_waves_1])\n",
    "max_sequence_len_2 = max([len(x) for x in all_waves_2])\n",
    "max_sequence_len_1 ,max_sequence_len_2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_waves_1 = np.array(pad_sequences(all_waves_1, maxlen=max_sequence_len_2, padding='post'))\n",
    "all_waves_2 = np.array(pad_sequences(all_waves_2, maxlen=max_sequence_len_2, padding='post'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_waves =  np.vstack((all_waves_1,all_waves_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs,tims,spectros = get_spectrograms(all_waves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 480)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectros[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectros = np.array(spectros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170, 129, 480)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectros = spectros.reshape(1170,129,480,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = np.array(labels_1)\n",
    "labels_2 = np.array(labels_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = [1 for i in labels_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2 = [0 for i in labels_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = np.array(labels_1).reshape(640,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2 = np.array(labels_2).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.vstack((labels_1,labels_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xe919987518>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(labels.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#labels = keras.utils.to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, Y, Y_test = train_test_split(spectros, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936, 129, 480, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (5,5), activation='relu',padding='same', input_shape=(X.shape[1], X.shape[2],1)),\n",
    "  tf.keras.layers.Conv2D(32,(5,5), activation='relu',padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Dropout((0.25)),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2)),\n",
    "  tf.keras.layers.Dropout((0.25)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout((0.5)),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam( epsilon=1e-08), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 129, 480, 32)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 129, 480, 32)      25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 240, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 240, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 240, 64)       18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 240, 64)       36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 120, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 120, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 245760)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               125829632 \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 125,912,033\n",
      "Trainable params: 125,912,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (936, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-5647fd00068a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lenovo_pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\lenovo_pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lenovo_pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2467\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2469\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lenovo_pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    657\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[0;32m    658\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                          \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m                          \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m                          \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (936, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "model.fit(X,Y,batch_size=128,epochs=200,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
